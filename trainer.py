"""
DFCRå®Œæ•´è®­ç»ƒè„šæœ¬ç¤ºä¾‹
åŒ…å«æ•°æ®åŠ è½½ã€è®­ç»ƒã€éªŒè¯å’Œä¿å­˜æ¨¡å‹
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from torch.optim.lr_scheduler import StepLR
import time
from pathlib import Path

from DFCR import DFCR, CaptchaDataset


class DFCRTrainer:
    """DFCRè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        optimizer: torch.optim.Optimizer,
        criterion: nn.Module,
        device: torch.device,
        save_dir: str = './checkpoints'
    ):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        self.best_val_acc = 0.0
        self.train_losses = []
        self.train_accs = []
        self.val_losses = []
        self.val_accs = []
    
    def train_epoch(self, epoch: int) -> tuple:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        start_time = time.time()
        
        for batch_idx, (images, labels) in enumerate(self.train_loader):
            images = images.to(self.device)
            
            # å°†labelsè½¬æ¢ä¸ºlist of tensors
            if not isinstance(labels, list):
                labels = [labels[:, i].to(self.device) for i in range(labels.shape[1])]
            else:
                labels = [label.to(self.device) for label in labels]
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(images)
            
            # è®¡ç®—æŸå¤±
            if isinstance(outputs, list):
                # å¤šè¾“å‡ºæƒ…å†µ
                loss = sum([self.criterion(output, label) 
                           for output, label in zip(outputs, labels)])
                
                # è®¡ç®—å‡†ç¡®ç‡ï¼ˆæ‰€æœ‰å­—ç¬¦éƒ½æ­£ç¡®æ‰ç®—æ­£ç¡®ï¼‰
                batch_correct = 0
                for i in range(len(images)):
                    all_correct = all([
                        outputs[j][i].argmax() == labels[j][i]
                        for j in range(len(outputs))
                    ])
                    if all_correct:
                        batch_correct += 1
                correct += batch_correct
            else:
                # å•è¾“å‡ºæƒ…å†µ
                loss = self.criterion(outputs, labels[0])
                correct += (outputs.argmax(1) == labels[0]).sum().item()
            
            total += len(images)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % 10 == 0:
                print(f'  Batch [{batch_idx+1}/{len(self.train_loader)}], '
                      f'Loss: {loss.item():.4f}')
        
        epoch_time = time.time() - start_time
        avg_loss = total_loss / len(self.train_loader)
        accuracy = correct / total
        
        print(f'Epoch {epoch} - è®­ç»ƒå®Œæˆ, è€—æ—¶: {epoch_time:.2f}s')
        print(f'  å¹³å‡æŸå¤±: {avg_loss:.4f}, å‡†ç¡®ç‡: {accuracy:.4f}')
        
        return avg_loss, accuracy
    
    def validate(self, epoch: int) -> tuple:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, labels in self.val_loader:
                images = images.to(self.device)
                
                if not isinstance(labels, list):
                    labels = [labels[:, i].to(self.device) for i in range(labels.shape[1])]
                else:
                    labels = [label.to(self.device) for label in labels]
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(images)
                
                # è®¡ç®—æŸå¤±
                if isinstance(outputs, list):
                    loss = sum([self.criterion(output, label) 
                               for output, label in zip(outputs, labels)])
                    
                    # è®¡ç®—å‡†ç¡®ç‡
                    batch_correct = 0
                    for i in range(len(images)):
                        all_correct = all([
                            outputs[j][i].argmax() == labels[j][i]
                            for j in range(len(outputs))
                        ])
                        if all_correct:
                            batch_correct += 1
                    correct += batch_correct
                else:
                    loss = self.criterion(outputs, labels[0])
                    correct += (outputs.argmax(1) == labels[0]).sum().item()
                
                total += len(images)
                total_loss += loss.item()
        
        avg_loss = total_loss / len(self.val_loader)
        accuracy = correct / total
        
        print(f'Epoch {epoch} - éªŒè¯ç»“æœ:')
        print(f'  å¹³å‡æŸå¤±: {avg_loss:.4f}, å‡†ç¡®ç‡: {accuracy:.4f}')
        
        return avg_loss, accuracy
    
    def save_checkpoint(self, epoch: int, val_acc: float, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'val_acc': val_acc,
            'train_losses': self.train_losses,
            'train_accs': self.train_accs,
            'val_losses': self.val_losses,
            'val_accs': self.val_accs
        }
        
        # ä¿å­˜æœ€æ–°æ¨¡å‹
        checkpoint_path = self.save_dir / f'checkpoint_epoch_{epoch}.pth'
        torch.save(checkpoint, checkpoint_path)
        print(f'ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}')
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜
        if is_best:
            best_path = self.save_dir / 'best_model.pth'
            torch.save(checkpoint, best_path)
            print(f'ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}')
    
    def train(self, num_epochs: int, scheduler=None):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f'\nå¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepochs')
        print(f'è®­ç»ƒé›†å¤§å°: {len(self.train_loader.dataset)}')
        print(f'éªŒè¯é›†å¤§å°: {len(self.val_loader.dataset)}')
        print(f'è®¾å¤‡: {self.device}\n')
        
        for epoch in range(1, num_epochs + 1):
            print(f'\n{"="*60}')
            print(f'Epoch {epoch}/{num_epochs}')
            print(f'{"="*60}')
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            self.train_losses.append(train_loss)
            self.train_accs.append(train_acc)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(epoch)
            self.val_losses.append(val_loss)
            self.val_accs.append(val_acc)
            
            # å­¦ä¹ ç‡è°ƒæ•´
            if scheduler is not None:
                scheduler.step()
                print(f'  å½“å‰å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.6f}')
            
            # ä¿å­˜æ¨¡å‹
            is_best = val_acc > self.best_val_acc
            if is_best:
                self.best_val_acc = val_acc
                print(f'  ğŸ‰ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {val_acc:.4f}')
            
            if epoch % 5 == 0 or is_best:
                self.save_checkpoint(epoch, val_acc, is_best)
        
        print(f'\n{"="*60}')
        print(f'è®­ç»ƒå®Œæˆï¼')
        print(f'æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}')
        print(f'{"="*60}\n')


def collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°ï¼Œå¤„ç†å¤šæ ‡ç­¾æƒ…å†µ"""
    images = torch.stack([item[0] for item in batch])
    labels = [item[1] for item in batch]
    
    # è½¬ç½®æ ‡ç­¾ï¼Œä½¿å…¶æˆä¸º [num_chars, batch_size]
    num_chars = len(labels[0])
    labels_transposed = [
        torch.tensor([labels[i][j] for i in range(len(labels))], dtype=torch.long)
        for j in range(num_chars)
    ]
    
    return images, labels_transposed


def setup_training(
    data_folder: str,
    char_set: str = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz',
    num_chars: int = 5,
    num_classes_per_char: int = 62,
    batch_size: int = 16,
    num_workers: int = 4,
    val_split: float = 0.2
):
    """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
    
    # åˆ›å»ºæ•°æ®é›†
    print("åŠ è½½æ•°æ®é›†...")
    full_dataset = CaptchaDataset(
        image_folder=data_folder,
        char_set=char_set,
        num_chars=num_chars
    )
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    val_size = int(len(full_dataset) * val_split)
    train_size = len(full_dataset) - val_size
    train_dataset, val_dataset = random_split(
        full_dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    print(f"æ•°æ®é›†æ€»æ•°: {len(full_dataset)}")
    print(f"è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True
    )
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºDFCRæ¨¡å‹...")
    model = DFCR(
        input_channels=3,
        num_classes_per_char=num_classes_per_char,
        num_chars=num_chars,
        growth_rate=32,
        dataset_type=1  # Dataset #1
    ).to(device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ€»å‚æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.SGD(
        model.parameters(),
        lr=0.001,
        momentum=0.9,
        nesterov=True,
        weight_decay=1e-4
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
    
    # æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    
    return model, train_loader, val_loader, optimizer, scheduler, criterion, device


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    config = {
        'data_folder': './data/captcha_images',  # æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        'char_set': '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz',
        'num_chars': 5,
        'num_classes_per_char': 62,
        'batch_size': 16,
        'num_workers': 4,
        'val_split': 0.2,
        'num_epochs': 100,
        'save_dir': './checkpoints'
    }
    
    # è®¾ç½®è®­ç»ƒç¯å¢ƒ
    model, train_loader, val_loader, optimizer, scheduler, criterion, device = setup_training(
        data_folder=config['data_folder'],
        char_set=config['char_set'],
        num_chars=config['num_chars'],
        num_classes_per_char=config['num_classes_per_char'],
        batch_size=config['batch_size'],
        num_workers=config['num_workers'],
        val_split=config['val_split']
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DFCRTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        optimizer=optimizer,
        criterion=criterion,
        device=device,
        save_dir=config['save_dir']
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(
        num_epochs=config['num_epochs'],
        scheduler=scheduler
    )


def inference_example(model_path: str, image_path: str, char_set: str):
    """æ¨ç†ç¤ºä¾‹"""
    from torchvision import transforms
    
    # åŠ è½½æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    model = DFCR(
        input_channels=3,
        num_classes_per_char=len(char_set),
        num_chars=5,
        growth_rate=32,
        dataset_type=1
    ).to(device)
    
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # å‡†å¤‡å›¾åƒ
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    image = Image.open(image_path).convert('RGB')
    image_tensor = transform(image).unsqueeze(0).to(device)
    
    # æ¨ç†
    with torch.no_grad():
        outputs = model(image_tensor)
    
    # è§£ç ç»“æœ
    char_to_idx = {char: idx for idx, char in enumerate(char_set)}
    idx_to_char = {idx: char for idx, char in enumerate(char_set)}
    
    predicted_text = ''
    if isinstance(outputs, list):
        for output in outputs:
            idx = output.argmax(1).item()
            predicted_text += idx_to_char[idx]
    else:
        idx = outputs.argmax(1).item()
        predicted_text = idx_to_char[idx]
    
    print(f"é¢„æµ‹ç»“æœ: {predicted_text}")
    return predicted_text


if __name__ == "__main__":
    print("""
    DFCRè®­ç»ƒè„šæœ¬
    
    ä½¿ç”¨æ–¹æ³•:
    1. å‡†å¤‡æ•°æ®: å°†CAPTCHAå›¾ç‰‡æ”¾åœ¨æ–‡ä»¶å¤¹ä¸­ï¼Œæ–‡ä»¶åä½œä¸ºæ ‡ç­¾ï¼ˆä¾‹å¦‚: ABC12.jpgï¼‰
    2. ä¿®æ”¹configä¸­çš„data_folderè·¯å¾„
    3. è¿è¡Œ: python train_dfcr.py
    
    æ¨ç†ä½¿ç”¨:
    predicted_text = inference_example(
        model_path='./checkpoints/best_model.pth',
        image_path='test_image.jpg',
        char_set='0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
    )
    """)
    
    # å–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œè®­ç»ƒ
    # main()